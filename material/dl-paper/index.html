<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.48" />
  <meta name="author" content="Linlin Chen (陈林林)">
  <meta name="description" content="Ph.D Candidate">

  
  
  
    
  
  
    
    
    <link rel="stylesheet" href="/css/highlight.min.css">
    
  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.0/css/academicons.min.css" integrity="sha512-GGGNUPDhnG8LEAEDsjqYIQns+Gu8RBs4j5XGlxl7UfRaZBhCCm5jenJkeJL8uPuOXGqgl8/H1gjlWQDRjd3cUQ==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:400,700%7CMerriweather%7CRoboto+Mono">
  <link rel="stylesheet" href="/css/hugo-academic.css">
  

  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-101350503-1', 'auto');
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  

  <link rel="alternate" href="https://llgeek.github.io/index.xml" type="application/rss+xml" title="Linlin Chen @ IIT">
  <link rel="feed" href="https://llgeek.github.io/index.xml" type="application/rss+xml" title="Linlin Chen @ IIT">

  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/apple-touch-icon.png">

  <link rel="canonical" href="https://llgeek.github.io/material/dl-paper/">

  

  <title>Deep Learning — Paper List | Linlin Chen @ IIT</title>

</head>
<body id="top" data-spy="scroll" data-target="#navbar-main" data-offset="71">

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="/">Linlin Chen @ IIT</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      <ul class="nav navbar-nav navbar-right">
        

        

        <li class="nav-item">
          <a href="/#about">
            
            <span>Home</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/#publications">
            
            <span>Publications</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/#experience">
            
            <span>Experience</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/#activities">
            
            <span>Activities</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/#materials">
            
            <span>Materials</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/blog/">
            
            <span>Blog</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/#contact">
            
            <span>Contact</span>
          </a>
        </li>

        
        

        
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  


  <div class="article-container">
    <h1 itemprop="name">Deep Learning — Paper List</h1>
    

<div class="article-metadata">

  <span class="article-date">
    <time datetime="2017-06-26 00:00:00 &#43;0000 UTC" itemprop="datePublished">
      Mon, Jun 26, 2017
    </time>
  </span>

  

  

  
  
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fllgeek.github.io%2fmaterial%2fdl-paper%2f"
         target="_blank">
        <i class="fa fa-facebook"></i>
      </a>
    </li>
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=Deep%20Learning%20%e2%80%94%20Paper%20List&amp;url=https%3a%2f%2fllgeek.github.io%2fmaterial%2fdl-paper%2f"
         target="_blank">
        <i class="fa fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fllgeek.github.io%2fmaterial%2fdl-paper%2f&amp;title=Deep%20Learning%20%e2%80%94%20Paper%20List"
         target="_blank">
        <i class="fa fa-linkedin"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=https%3a%2f%2fllgeek.github.io%2fmaterial%2fdl-paper%2f&amp;title=Deep%20Learning%20%e2%80%94%20Paper%20List"
         target="_blank">
        <i class="fa fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=Deep%20Learning%20%e2%80%94%20Paper%20List&amp;body=https%3a%2f%2fllgeek.github.io%2fmaterial%2fdl-paper%2f">
        <i class="fa fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>

    <div class="article-style" itemprop="articleBody">
      

<h3 id="generative-model">Generative Model</h3>

<ol>
<li>Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A. and Bengio, Y., 2014. <a href="http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf" target="_blank">Generative adversarial nets</a>. In Advances in neural information processing systems</li>
<li>Kingma, D.P. and Welling, M., 2013. <a href="https://arxiv.org/pdf/1312.6114.pdf" target="_blank">Auto-encoding variational bayes</a>. arXiv preprint arXiv:1312.6114.</li>
<li>Bengio, Y., Yao, L., Alain, G. and Vincent, P., 2013. <a href="http://papers.nips.cc/paper/5023-generalized-denoising-auto-encoders-as-generative-models.pdf" target="_blank">Generalized denoising auto-encoders as generative models</a>. In Advances in Neural Information Processing Systems (pp. 899-907).</li>
<li>Vincent, P., Larochelle, H., Bengio, Y. and Manzagol, P.A., 2008, July. <a href="../DAE-ICML.pdf">Extracting and composing robust features with denoising autoencoders</a>. In Proceedings of the 25th international conference on Machine learning (pp. 1096-1103). ACM.</li>
</ol>

<hr />

<h3 id="generative-adversarial-networks">Generative Adversarial Networks</h3>

<ol>
<li>Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A. and Bengio, Y. <a href="http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf" target="_blank">Generative adversarial nets</a>, NIPS (2014).</li>
<li>Goodfellow, Ian <a href="https://arxiv.org/pdf/1701.00160" target="_blank">NIPS 2016 Tutorial: Generative Adversarial Networks</a>, NIPS (2016).</li>
<li>Radford, A., Metz, L. and Chintala, S., <a href="https://arxiv.org/pdf/1511.06434.pdf" target="_blank">Unsupervised representation learning with deep convolutional generative adversarial networks.</a> arXiv preprint arXiv:1511.06434. (2015)</li>
<li>Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., &amp; Chen, X. <a href="https://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf" target="_blank">Improved techniques for training gans.</a> NIPS (2016).</li>
<li>Chen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I., &amp; Abbeel, P. <a href="https://papers.nips.cc/paper/6399-infogan-interpretable-representation-learning-by-information-maximizing-generative-adversarial-nets.pdf" target="_blank">InfoGAN: Interpretable Representation Learning by Information Maximization Generative Adversarial Nets</a>, NIPS (2016).</li>
<li>Zhao, Junbo, Michael Mathieu, and Yann LeCun. <a href="https://arxiv.org/pdf/1609.03126" target="_blank">Energy-based generative adversarial network.</a> arXiv preprint arXiv:1609.03126 (2016).</li>
<li>Mirza, Mehdi, and Simon Osindero. <a href="https://arxiv.org/pdf/1411.1784" target="_blank">Conditional generative adversarial nets.</a> arXiv preprint arXiv:1411.1784 (2014).</li>
<li>Isola, P., Zhu, J. Y., Zhou, T., &amp; Efros, A. A. <a href="https://arxiv.org/pdf/1611.07004" target="_blank">Image-to-image translation with conditional adversarial networks.</a> arXiv preprint arXiv:1611.07004. (2016).</li>
<li>Reed, S., Akata, Z., Yan, X., Logeswaran, L., Schiele, B., &amp; Lee, H. <a href="http://www.jmlr.org/proceedings/papers/v48/reed16.pdf" target="_blank">Generative adversarial text to image synthesis.</a> JMLR (2016).</li>
<li>Antipov, G., Baccouche, M., &amp; Dugelay, J. L. <a href="https://arxiv.org/pdf/1702.01983.pdf" target="_blank">Face Aging With Conditional Generative Adversarial Networks.</a> arXiv preprint arXiv:1702.01983. (2017).</li>
<li>Liu, Ming-Yu, and Oncel Tuzel. <a href="https://papers.nips.cc/paper/6544-coupled-generative-adversarial-networks.pdf" target="_blank">Coupled generative adversarial networks.</a> NIPS (2016).</li>
<li>Denton, E.L., Chintala, S. and Fergus, R., 2015. <a href="http://papers.nips.cc/paper/5773-deep-generative-image-models-using-a-laplacian-pyramid-of-adversarial-networks.pdf" target="_blank">Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks.</a> NIPS (2015).</li>
<li>Dumoulin, V., Belghazi, I., Poole, B., Lamb, A., Arjovsky, M., Mastropietro, O., &amp; Courville, A. <a href="https://arxiv.org/pdf/1606.00704" target="_blank">Adversarially learned inference.</a> arXiv preprint arXiv:1606.00704 (2016).</li>
</ol>

<hr />

<h3 id="variational-autoencoders">Variational Autoencoders</h3>

<ol>
<li>D. Kingma, M. Welling, <a href="https://arxiv.org/abs/1312.6114" target="_blank">Auto-Encoding Variational Bayes</a>, ICLR, 2014</li>
<li>Carl Doersch, <a href="https://arxiv.org/abs/1606.05908" target="_blank">Tutorial on Variational Autoencoders</a> arXiv, 2016</li>
<li>Xinchen Yan, Jimei Yang, Kihyuk Sohn, Honglak Lee, <a href="https://arxiv.org/abs/1512.00570" target="_blank">Attribute2Image: Conditional Image Generation from Visual Attributes</a>, ECCV, 2016</li>
<li>Jacob Walker, Carl Doersch, Abhinav Gupta, Martial Hebert, <a href="https://arxiv.org/abs/1606.07873" target="_blank">An Uncertain Future: Forecasting from Static Images using Variational Autoencoders</a>, ECCV, 2016</li>
<li>Anh Nguyen, Jason Yosinski, Yoshua Bengio, Alexey Dosovitskiy, Jeff Clune, <a href="https://arxiv.org/abs/1612.00005" target="_blank">Plug &amp; Play Generative Networks: Conditional Iterative Generation of Images in Latent Space</a>, arXiv, 2016</li>
<li>Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, Brendan Frey, <a href="https://arxiv.org/abs/1511.05644" target="_blank">Adversarial Autoencoders</a>, ICLR, 2016</li>
<li>Anders Boesen Lindbo Larsen, SÃ¸ren Kaae SÃ¸nderby, Hugo Larochelle, Ole Winther, <a href="https://arxiv.org/abs/1512.09300" target="_blank">Autoencoding beyond pixels using a learned similarity metric</a>, ICML, 2016</li>
<li>Aditya Deshpande, Jiajun Lu, Mao-Chuang Yeh, David Forsyth, <a href="https://arxiv.org/abs/1612.01958" target="_blank">Learning Diverse Image Colorization</a>, arXiv, 2016</li>
<li>Jiajun Lu, Aditya Deshpande, David Forsyth, <a href="https://arxiv.org/abs/1612.00132" target="_blank">CDVAE: Co-embedding Deep Variational Auto Encoder for Conditional Variational Generation</a>, arXiv, 2016</li>
<li>Diederik P. Kingma, Danilo J. Rezende, Shakir Mohamed, Max Welling, <a href="https://arxiv.org/abs/1406.5298" target="_blank">Semi-Supervised Learning with Deep Generative Models</a>, NIPS, 2014</li>
<li>Lars MaalÃ¸e, Casper Kaae SÃ¸nderby, SÃ¸ren Kaae SÃ¸nderby, Ole Winther, <a href="https://arxiv.org/abs/1602.05473" target="_blank">Auxiliary Deep Generative Models</a> arXiv, 2016</li>
<li>Raymond Yeh, Ziwei Liu, Dan B Goldman, Aseem Agarwala, <a href="http://slazebni.cs.illinois.edu/spring17/reading_lists.html" target="_blank">Semantic Facial Expression Editing using Autoencoded Flow</a> arXiv, 2016</li>
</ol>

<hr />

<h3 id="model-compression">Model Compression</h3>

<ol>
<li>Denton, Emily L., et al. <a href="http://papers.nips.cc/paper/5544-exploiting-linear-structure-within-convolutional-networks-for-efficient-evaluation.pdf" target="_blank">&ldquo;Exploiting linear structure within convolutional networks for efficient evaluation.&rdquo;</a> Advances in Neural Information Processing Systems. 2014.</li>
<li>Jin, Jonghoon, Aysegul Dundar, and Eugenio Culurciello. <a href="https://arxiv.org/pdf/1412.5474v4.pdf" target="_blank">&ldquo;Flattened convolutional neural networks for feedforward acceleration.&rdquo;</a> arXiv preprint arXiv:1412.5474 (2014).</li>
<li>Gong, Yunchao, et al. <a href="https://arxiv.org/pdf/1412.6115v1.pdf" target="_blank">&ldquo;Compressing deep convolutional networks using vector quantization.&rdquo;</a> arXiv preprint arXiv:1412.6115 (2014).</li>
<li>Han, Song, et al. <a href="http://papers.nips.cc/paper/5784-learning-both-weights-and-connections-for-efficient-neural-network.pdf" target="_blank">&ldquo;Learning both weights and connections for efficient neural network.&rdquo;</a> Advances in Neural Information Processing Systems. 2015.</li>
<li>Guo, Yiwen, Anbang Yao, and Yurong Chen. <a href="https://arxiv.org/pdf/1608.04493v2.pdf" target="_blank">&ldquo;Dynamic Network Surgery for Efficient DNNs.&rdquo;</a> Advances In Neural Information Processing Systems. 2016.</li>
<li>Gupta, Suyog, et al. <a href="http://www.jmlr.org/proceedings/papers/v37/gupta15.pdf" target="_blank">&ldquo;Deep Learning with Limited Numerical Precision.&rdquo;</a> ICML. 2015.</li>
<li>Courbariaux, Matthieu, Yoshua Bengio, and Jean-Pierre David. <a href="http://papers.nips.cc/paper/5647-binaryconnect-training-deep-neural-networks-with-binary-weights-during-propagations.pdf" target="_blank">&ldquo;Binaryconnect: Training deep neural networks with binary weights during propagations.&rdquo;</a> Advances in Neural Information Processing Systems. 2015.</li>
<li>Courbariaux, Matthieu, et al. <a href="https://arxiv.org/pdf/1602.02830v3.pdf" target="_blank">&ldquo;Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1.&rdquo;</a> arXiv preprint arXiv:1602.02830 (2016).</li>
<li>Han, Song, Huizi Mao, and William J. Dally. <a href="https://arxiv.org/pdf/1510.00149v5.pdf" target="_blank">&ldquo;Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding.&rdquo;</a> arXiv preprint arXiv:1510.00149 (2015).</li>
<li>Iandola, Forrest N., et al. <a href="https://arxiv.org/pdf/1602.07360v4.pdf" target="_blank">&ldquo;SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and&lt; 0.5 MB model size.&rdquo;</a> arXiv preprint arXiv:1602.07360 (2016).</li>
</ol>

<hr />

<h3 id="rnn">RNN</h3>

<ol>
<li>R. Pascanu, T. Mikolov, and Y. Bengio, <a href="http://www.jmlr.org/proceedings/papers/v28/pascanu13.pdf" target="_blank">On the difficulty of training recurrent neural networks</a>, ICML 2013</li>
<li>S. Hochreiter, and J. Schmidhuber, J., <a href="http://web.eecs.utk.edu/~itamar/courses/ECE-692/Bobby_paper1.pdf" target="_blank">Long short-term memory</a>, Neural computation, 1997 9(8), pp.1735-1780</li>
<li>F.A. Gers, and J. Schmidhuber, J., <a href="ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf" target="_blank">Recurrent nets that time and count</a>, IJCNN 2000</li>
<li>K. Greff , R.K. Srivastava, J. KoutnÃ­k, B.R. Steunebrink, and J. Schmidhuber, <a href="https://arxiv.org/pdf/1503.04069.pdf" target="_blank">LSTM: A search space odyssey</a>, IEEE transactions on neural networks and learning systems, 2016</li>
<li>K. Cho, B. Van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio, <a href="https://arxiv.org/pdf/1406.1078.pdf" target="_blank">Learning phrase representations using RNN encoder-decoder for statistical machine translation</a>, ACL 2014</li>
<li>R. Jozefowicz, W. Zaremba, and I. Sutskever, <a href="http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf" target="_blank">An empirical exploration of recurrent network architectures</a>, JMLR 2015</li>
</ol>

<hr />

<h3 id="recurrent-architectures-lstm-gru-rnn">Recurrent Architectures: LSTM, GRU, RNN</h3>

<ol>
<li>Survey Papers

<ul>
<li>Lipton, Zachary C., John Berkowitz, and Charles Elkan. <a href="https://arxiv.org/abs/1506.00019" target="_blank">A critical review of recurrent neural networks for sequence learning</a>, arXiv preprint arXiv:1506.00019 (2015).</li>
<li>Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. <a href="https://www.deeplearningbook.org/contents/rnn.html" target="_blank">Chapter 10: Sequence Modeling: Recurrent and Recursive Nets</a>. MIT Press, 2016.</li>
</ul></li>
<li>Training

<ul>
<li>Semeniuta, Stanislau, Aliaksei Severyn, and Erhardt Barth. <a href="https://arxiv.org/abs/1609.01704" target="_blank">Recurrent dropout without memory loss. </a>arXiv preprint arXiv:1603.05118 (2016).</li>
<li>Arjovsky, Martin, Amar Shah, and Yoshua Bengio. <a href="https://arxiv.org/abs/1511.06464" target="_blank">Unitary evolution recurrent neural networks.</a> arXiv preprint arXiv:1511.06464 (2015).</li>
<li>Le, Quoc V., Navdeep Jaitly, and Geoffrey E. Hinton. <a href="https://arxiv.org/abs/1504.00941" target="_blank">A simple way to initialize recurrent networks of rectified linear units.</a> arXiv preprint arXiv:1504.00941 (2015).</li>
<li>Cooijmans, Tim, et al. <a href="https://arxiv.org/abs/1603.09025" target="_blank">Recurrent batch normalization.</a> arXiv preprint arXiv:1603.09025 (2016).</li>
</ul></li>
<li>Architectural Complexity Measures

<ul>
<li>Zhang, Saizheng, et al, <a href="https://arxiv.org/abs/1602.08210" target="_blank">Architectural Complexity Measures of Recurrent Neural Networks. </a>Advances in Neural Information Processing Systems. 2016.</li>
<li>Pascanu, Razvan, et al. <a href="https://arxiv.org/abs/1312.6026" target="_blank">How to construct deep recurrent neural networks.</a> arXiv preprint arXiv:1312.6026 (2013).</li>
</ul></li>
<li>RNN Variants

<ul>
<li>Zilly, Julian Georg, et al. <a href="https://arxiv.org/abs/1607.03474" target="_blank">Recurrent highway networks. </a>arXiv preprint arXiv:1607.03474 (2016)</li>
<li>Chung, Junyoung, Sungjin Ahn, and Yoshua Bengio. <a href="https://arxiv.org/abs/1609.01704" target="_blank">Hierarchical multiscale recurrent neural networks</a>, arXiv preprint arXiv:1609.01704 (2016).</li>
</ul></li>
<li>Visualization

<ul>
<li>Karpathy, Andrej, Justin Johnson, and Li Fei-Fei. <a href="https://arxiv.org/pdf/1506.02078.pdf" target="_blank">Visualizing and understanding recurrent networks.</a> arXiv preprint arXiv:1506.02078 (2015).</li>
<li>Hendrik Strobelt, Sebastian Gehrmann, Bernd Huber, Hanspeter Pfister, Alexander M. Rush. <a href="http://lstm.seas.harvard.edu/" target="_blank">LSTMVis: Visual Analysis for RNN</a>, arXiv preprint arXiv:1606.07461 (2016).</li>
</ul></li>
</ol>

<h3 id="advanced-cnn-architectures">Advanced CNN Architectures</h3>

<ol>
<li>K. He, X. Zhang, S. Ren, and J. Sun, <a href="https://arxiv.org/abs/1512.03385" target="_blank">Deep Residual Learning for Image Recognition</a>, CVPR 2016</li>
<li>Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, <a href="https://arxiv.org/abs/1603.05027" target="_blank">Identity Mappings in Deep Residual Networks</a>, ECCV 2016</li>
<li>Gao Huang, Zhuang Liu, Kilian Q. Weinberger, Laurens van der Maaten: <a href="https://arxiv.org/pdf/1608.06993v3.pdf" target="_blank">Densely Connected Convolutional Networks</a></li>
<li>Andreas Veit, Michael Wilber, Serge Belongie, <a href="https://arxiv.org/pdf/1605.06431v2.pdf" target="_blank">Residual Networks Behave Like Ensembles of Relatively Shallow Networks</a>, NIPS 2016</li>
<li>Klaus Greff, Rupesh K. Srivastava &amp; JÃ¼rgen Schmidhuber, <a href="https://arxiv.org/pdf/1612.07771v1.pdf" target="_blank">Highway and Residual Networks Learn Unrolled Iterative Estimation</a></li>
</ol>

<hr />

<h3 id="advanced-training-techniques">Advanced Training Techniques</h3>

<ol>
<li>D. Kingma, and J. Ba, <a href="https://arxiv.org/pdf/1412.6980.pdf" target="_blank">Adam: a method for stochastic optimization</a>, ICLR 2015</li>
<li>J. Dean et al., <a href="https://static.googleusercontent.com/media/research.google.com/en//archive/large_deep_networks_nips2012.pdf" target="_blank">Large scale distributed deep networks</a>, NIPS 2012</li>
<li>N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, <a href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf" target="_blank">Dropout: a simple way to prevent neural networks from overfitting</a>, JMLR 2014</li>
<li>S. Ioffe and C. Szegedy, <a href="https://arxiv.org/pdf/1502.03167v3.pdf" target="_blank">Batch normalization: accelerating deep network training by reducing internal covariate shift</a>, ICML 2015</li>
<li>K. He, X. Zhang, S. Ren, and J. Sun, <a href="https://arxiv.org/pdf/1502.01852v1.pdf" target="_blank">Delving deep into rectifiers: surpassing human-level performance on ImageNet classification</a>, ICCV 2015</li>
</ol>

<hr />

<h3 id="object-detection">Object Detection</h3>

<ol>
<li>Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra, <a href="https://people.eecs.berkeley.edu/~rbg/papers/r-cnn-cvpr.pdf" target="_blank">Rich feature hierarchies for accurate object detection and semantic segmentation</a>, CVPR 2014</li>
<li>He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian, <a href="https://arxiv.org/pdf/1406.4729v4.pdf" target="_blank">Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</a>, ECCV 2014</li>
<li>Jifeng Dai, Yi Li, Kaiming He, Jian Sun R-FCN: <a href="http://papers.nips.cc/paper/6465-r-fcn-object-detection-via-region-based-fully-convolutional-networks.pdf" target="_blank">Object Detection via Region-based Fully Convolutional Networks </a>, NIPS 2016</li>
<li>Girshick, Ross, <a href="https://arxiv.org/pdf/1504.08083v2.pdf" target="_blank">Fast R-CNN</a>, ICCV 2015</li>
<li>Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian, <a href="https://arxiv.org/pdf/1506.01497v3.pdf" target="_blank">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a>, CVPR 2015</li>
<li>Erhan, Dumitru and Szegedy, Christian and Toshev, Alexander and Anguelov, Dragomir, <a href="https://pdfs.semanticscholar.org/0674/792f5edac72b77fb1297572c15b153576418.pdf" target="_blank">Scalable Object Detection using Deep Neural Networks</a>, CVPR 2014</li>
<li>Bell, Sean and Lawrence Zitnick, C and Bala, Kavita and Girshick, Ross, <a href="https://arxiv.org/pdf/1512.04143v1.pdf" target="_blank">Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks</a>, CVPR 2016</li>
<li>Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali, <a href="http://pjreddie.com/media/files/papers/yolo.pdf" target="_blank">You Only Look Once: Unified, Real-Time Object Detection</a>, CVPR 2016</li>
<li>Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C, <a href="https://arxiv.org/pdf/1512.02325v5.pdf" target="_blank">SSD: Single Shot MultiBox Detector</a>, ECCV 2016</li>
<li>Tsung-Yi Lin, Piotr DollÃ¡r, Ross Girshick, Kaiming He, Bharath Hariharan, Serge Belongie, <a href="https://arxiv.org/pdf/1612.03144.pdf" target="_blank">Feature Pyramid Networks for Object Detection</a>, arXiv 2016</li>
<li>Huang, Jonathan and Rathod, Vivek and Sun, Chen and Zhu, Menglong and Korattikara, Anoop and Fathi, Alireza and Fischer, Ian and Wojna, Zbigniew and Song, Yang and Guadarrama, Sergio and others, <a href="https://arxiv.org/pdf/1611.10012v1.pdf" target="_blank">Speed/accuracy trade-offs for modern convolutional object detectors</a>, arXiv 2016</li>
</ol>

<hr />

<h3 id="adversarial-samples">Adversarial Samples</h3>

<ol>
<li>Matthew D. Zeiler and Rob Fergus, <a href="https://arxiv.org/abs/1311.2901" target="_blank">Visualizing and Understanding Convolutional Networks</a>, ECCV 2014</li>
<li>Karen Simonyan, Andrea Vedaldi, Andrew Zisserman, <a href="https://arxiv.org/abs/1312.6034" target="_blank">Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps</a>, arXiv:1312.6034v2</li>
<li>Alexey Dosovitskiy and Thomas Brox, <a href="https://arxiv.org/abs/1506.02753" target="_blank">Inverting Visual Representations with Convolutional Networks</a>, CVPR 2016</li>
<li>Anh Nguyen, Jason Yosinski, and Jeff Clune, <a href="https://arxiv.org/abs/1412.1897" target="_blank">Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images</a>, CVPR 2015</li>
<li>Christian Szegedy, et al., <a href="https://arxiv.org/abs/1312.6199" target="_blank">Intriguing properties of neural networks</a>, arXiv preprint arXiv:1312.6199v4</li>
<li>Seyed-Mohsen Moosavi-Dezfooli, et al, <a href="https://arxiv.org/abs/1610.08401" target="_blank">Universal adversarial perturbations</a>, arXiv preprint arXiv:1610.08401v2</li>
<li>Ian J. Goodfellow, et al, <a href="https://arxiv.org/abs/1412.6572" target="_blank">Explaining and Harnessing Adversarial Examples</a>, arXiv preprint arXiv:1412.6572</li>
<li>A. Kurakin et al., <a href="https://arxiv.org/pdf/1607.02533.pdf" target="_blank">Adversarial examples in the physical world</a>, ICLR 2017</li>
<li>D. Krotov and J. Hopfield, <a href="https://arxiv.org/pdf/1701.00939.pdf" target="_blank">Dense Associative Memory is Robust to Adversarial Inputs</a>, arXiv preprint arXiv:1701.00939</li>
</ol>

<hr />

    </div>
  </div>

</article>

<div class="container">
  <nav>
  <ul class="pager">
    
    <li class="previous"><a href="https://llgeek.github.io/material/dl-tool/"><span
      aria-hidden="true">&larr;</span> Deep Learning — Tools</a></li>
    

    
    <li class="next"><a href="https://llgeek.github.io/material/dl-video/">Deep Learning — Online Course <span
      aria-hidden="true">&rarr;</span></a></li>
    
  </ul>
</nav>

</div>

<div class="article-container">
  

</div>

<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      &copy; Linlin Chen. Last Updated: Aug. 2018 &middot; 

      Powered by the <a href="https://github.com/gcushen/hugo-academic" target="_blank">Academic
      theme</a> for <a href="http://gohugo.io" target="_blank">Hugo</a>.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>

    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/1.12.4/jquery.min.js" integrity="sha512-jGsMH83oKe9asCpkOVkBnUrDDTp8wl+adkB2D+//JtlxO4SrLoJdhbOysIFQJloQFD+C4Fl1rMsQZF76JjV0eQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.2/imagesloaded.pkgd.min.js" integrity="sha512-iHzEu7GbSc705hE2skyH6/AlTpOfBmkx7nUqTLGzPYR+C1tRaItbRlJ7hT/D3YQ9SV0fqLKzp4XY9wKulTBGTw==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/1.19.1/TweenMax.min.js" integrity="sha512-Z5heTz36xTemt1TbtbfXtTq5lMfYnOkXM2/eWcTTiLU01+Sw4ku1i7vScDc8fWhrP2abz9GQzgKH5NGBLoYlAw==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/1.19.1/plugins/ScrollToPlugin.min.js" integrity="sha512-CDeU7pRtkPX6XJtF/gcFWlEwyaX7mcAp5sO3VIu/ylsdR74wEw4wmBpD5yYTrmMAiAboi9thyBUr1vXRPA7t0Q==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
    
    <script src="/js/hugo-academic.js"></script>
    

    
    
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.9.0/highlight.min.js"></script>

      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
    </script>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML" integrity="sha512-tOav5w1OjvsSJzePRtt2uQPFwBoHt1VZcUq8l8nm5284LEKE9FSJBQryzMBzHxY5P0zRdNqEcpLIRVYFNgu1jw==" crossorigin="anonymous"></script>
    
    

  </body>
</html>

